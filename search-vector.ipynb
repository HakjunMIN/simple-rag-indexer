{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType, Vector\n",
    "\n",
    "load_dotenv()\n",
    "# Replace these with your own values, either in environment variables or directly here\n",
    "AZURE_STORAGE_ACCOUNT = os.environ.get(\"AZURE_STORAGE_ACCOUNT\")\n",
    "AZURE_STORAGE_CONTAINER = os.environ.get(\"AZURE_STORAGE_CONTAINER\")\n",
    "AZURE_SEARCH_SERVICE = os.environ.get(\"AZURE_SEARCH_SERVICE\")\n",
    "AZURE_SEARCH_INDEX = os.environ.get(\"AZURE_SEARCH_INDEX\") \n",
    "AZURE_OPENAI_SERVICE = os.environ.get(\"AZURE_OPENAI_SERVICE\") \n",
    "AZURE_OPENAI_GPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_GPT_DEPLOYMENT\") \n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\") \n",
    "AZURE_OPENAI_EMB_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_EMB_DEPLOYMENT\")\n",
    "AZURE_SEARCH_SERVICE_KEY = os.environ.get(\"AZURE_SEARCH_SERVICE_KEY\") \n",
    "\n",
    "KB_FIELDS_CONTENT = os.environ.get(\"KB_FIELDS_CONTENT\") or \"content\"\n",
    "KB_FIELDS_CATEGORY = os.environ.get(\"KB_FIELDS_CATEGORY\") or \"category\"\n",
    "KB_FIELDS_SOURCEPAGE = os.environ.get(\"KB_FIELDS_SOURCEPAGE\") or \"sourcepage\"\n",
    "\n",
    "\n",
    "# Used by the OpenAI SDK\n",
    "openai.api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "openai.api_version = \"2023-06-01-preview\"\n",
    "openai.api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\") \n",
    "# Comment these two lines out if using keys, set your API key in the OPENAI_API_KEY environment variable and set openai.api_type = \"azure\" instead\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# Set up clients for Cognitive Search and Storage\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=AzureKeyCredential(AZURE_SEARCH_SERVICE_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT uses a particular set of tokens to indicate turns in conversations\n",
    "prompt_prefix = \"\"\"<|im_start|>system\n",
    "어시스턴트는 검색 결과에서 나온 문서에 대한 질문을 도와줍니다. 답변은 간결하게 작성하세요.\n",
    "Source에 나오지 않은 내용은 모른다고 대답합니다. Source결과에 나온 내용을 잘 읽어보세요.\n",
    "표 형식의 정보는 HTML 테이블로 반환합니다. 마크다운 형식은 반환하지 마세요.\n",
    "각 출처에는 이름 뒤에 콜론과 실제 정보가 있으며, 응답에 사용하는 각 사실에 대한 출처 이름을 항상 포함하세요. 소스를 참조할 때는 대괄호를 사용합니다(예: [info1.txt]). 소스를 결합하지 말고 각 소스를 개별적으로 나열합니다(예: [info1.txt][info2.pdf]).\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "<|im_end|>\"\"\"\n",
    "\n",
    "turn_prefix = \"\"\"\n",
    "<|im_start|>user\n",
    "\"\"\"\n",
    "\n",
    "turn_suffix = \"\"\"\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt_history = turn_prefix\n",
    "\n",
    "history = []\n",
    "\n",
    "summary_prompt_template = \"\"\"아래는 지금까지의 대화 요약과 사용자가 검색결과에서 검색하여 답변해야 하는 새로운 질문입니다. 대화와 새 질문을 기반으로 검색 쿼리를 생성합니다.\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching: 인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "-------------------\n",
      "\n",
      "\n",
      "-------------------\n",
      "Prompt:\n",
      "<|im_start|>system\n",
      "어시스턴트는 검색 결과에서 나온 문서에 대한 질문을 도와줍니다. 답변은 간결하게 작성하세요.\n",
      "Source에 나오지 않은 내용은 모른다고 대답합니다. Source결과에 나온 내용을 잘 읽어보세요.\n",
      "표 형식의 정보는 HTML 테이블로 반환합니다. 마크다운 형식은 반환하지 마세요.\n",
      "각 출처에는 이름 뒤에 콜론과 실제 정보가 있으며, 응답에 사용하는 각 사실에 대한 출처 이름을 항상 포함하세요. 소스를 참조할 때는 대괄호를 사용합니다(예: [info1.txt]). 소스를 결합하지 말고 각 소스를 개별적으로 나열합니다(예: [info1.txt][info2.pdf]).\n",
      "Sources:\n",
      "\n",
      "\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "user: 인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "-------------------\n",
      "assistant: 인터넷 형광펜 서비스는 웹 브라우저 상에서 PDF나 웹페이지 등의 문서를 열어서 형광펜으로 강조하거나 메모를 추가할 수 있는 서비스입니다. 이를 통해 사용자는 문서를 보다 효율적으로 읽고, 필요한 부분을 쉽게 찾을 수 있습니다.\n",
      "\n",
      "대표적인 인터넷 형광펜 서비스로는 Kami, Hypothesis, Notable PDF 등이 있습니다. 이들 서비스는 각각 다양한 기능을 제공하며, 무료 또는 유료로 이용할 수 있습니다.\n",
      "\n",
      "Kami는 Google Drive와 연동되어 있어, Google Drive에 저장된 문서를 쉽게 열어서 형광펜으로 강조하거나 메모를 추가할 수 있습니다. Hypothesis는 학술 논문 등의 문서에 대한 협업 리뷰를 지원하며, Notable PDF는 형광펜 기능 뿐만 아니라 PDF 문서에 주석을 추가할 수 있는 기능도 제공합니다.\n",
      "\n",
      "[출처: https://www.kamiapp.com/, https://web.hypothes.is/, https://notablepdf.com/]\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell multiple times updating user_input to accumulate chat history\n",
    "user_input = \"인터넷 형광펜 서비스 소개좀 부탁해요\"\n",
    "\n",
    "# Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
    "exclude_category = None\n",
    "\n",
    "if len(history) > 0:\n",
    "    completion = openai.Completion.create(\n",
    "        engine=AZURE_OPENAI_GPT_DEPLOYMENT,\n",
    "        prompt=summary_prompt_template.format(summary=\"\\n\".join(history), question=user_input),\n",
    "        temperature=1,\n",
    "        max_tokens=32,\n",
    "        stop=[\"\\n\"])\n",
    "    search = completion.choices[0].text\n",
    "else:\n",
    "    search = user_input\n",
    "\n",
    "# Use Azure OpenAI to compute an embedding for the query\n",
    "query_vector = openai.Embedding.create(engine=AZURE_OPENAI_EMB_DEPLOYMENT, input=search)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "print(\"Searching:\", search)\n",
    "print(\"-------------------\")\n",
    "filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
    "\n",
    "# 1. only semantic search\n",
    "\n",
    "# r = search_client.search(search, \n",
    "#                          filter=filter,\n",
    "#                          query_type=QueryType.SEMANTIC, \n",
    "#                          query_language=\"en-us\", \n",
    "#                          query_speller=\"lexicon\", \n",
    "#                          semantic_configuration_name=\"default\", \n",
    "#                          top=3)\n",
    "\n",
    "# 2. Hybrid search with semantic search and vector reranking\n",
    "\n",
    "# r = search_client.search(search, \n",
    "#                          filter=filter,\n",
    "#                          query_type=QueryType.SEMANTIC, \n",
    "#                          query_language=\"en-us\", \n",
    "#                          query_speller=\"lexicon\", \n",
    "#                          semantic_configuration_name=\"default\", \n",
    "#                          top=3,\n",
    "#                          vector=query_vector, \n",
    "#                          top_k=50 if query_vector else None, \n",
    "#                          vector_fields=\"embedding\" if query_vector else None)\n",
    "\n",
    "# 3. Vector search only\n",
    "r = search_client.search(search, top=3, vector=query_vector, top_k=50 if query_vector else None, vector_fields=\"embedding\" if query_vector else None)\n",
    "\n",
    "results = [doc[KB_FIELDS_SOURCEPAGE] + \": \" + doc[KB_FIELDS_CONTENT].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in r]\n",
    "content = \"\\n\".join(results)\n",
    "\n",
    "prompt = prompt_prefix.format(sources=content) + prompt_history + user_input + turn_suffix\n",
    "\n",
    "print(\"\\n-------------------\\n\".join(history))\n",
    "print(\"\\n-------------------\\nPrompt:\\n\" + prompt)\n",
    "\n",
    "completion = openai.Completion.create(\n",
    "    engine=AZURE_OPENAI_CHATGPT_DEPLOYMENT, \n",
    "    prompt=prompt, \n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
    "\n",
    "prompt_history += user_input + turn_suffix + completion.choices[0].text + \"\\n<|im_end|>\" + turn_prefix\n",
    "history.append(\"user: \" + user_input)\n",
    "history.append(\"assistant: \" + completion.choices[0].text)\n",
    "\n",
    "print(\"\\n-------------------\\n\".join(history))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40b9fc8dfc687e53ddb074d322e19207ef9cf3db51c580aef67976913dea803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

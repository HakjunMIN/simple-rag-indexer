{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType, Vector\n",
    "\n",
    "load_dotenv()\n",
    "# Replace these with your own values, either in environment variables or directly here\n",
    "AZURE_STORAGE_ACCOUNT = os.environ.get(\"AZURE_STORAGE_ACCOUNT\")\n",
    "AZURE_STORAGE_CONTAINER = os.environ.get(\"AZURE_STORAGE_CONTAINER\")\n",
    "AZURE_SEARCH_SERVICE = os.environ.get(\"AZURE_SEARCH_SERVICE\")\n",
    "AZURE_SEARCH_INDEX = os.environ.get(\"AZURE_SEARCH_INDEX\") \n",
    "AZURE_OPENAI_SERVICE = os.environ.get(\"AZURE_OPENAI_SERVICE\") \n",
    "AZURE_OPENAI_GPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_GPT_DEPLOYMENT\") \n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\") \n",
    "AZURE_OPENAI_EMB_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_EMB_DEPLOYMENT\")\n",
    "AZURE_SEARCH_SERVICE_KEY = os.environ.get(\"AZURE_SEARCH_SERVICE_KEY\") \n",
    "\n",
    "KB_FIELDS_CONTENT = os.environ.get(\"KB_FIELDS_CONTENT\") or \"content\"\n",
    "KB_FIELDS_CATEGORY = os.environ.get(\"KB_FIELDS_CATEGORY\") or \"category\"\n",
    "KB_FIELDS_SOURCEPAGE = os.environ.get(\"KB_FIELDS_SOURCEPAGE\") or \"sourcepage\"\n",
    "\n",
    "\n",
    "# Used by the OpenAI SDK\n",
    "openai.api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "openai.api_version = \"2023-06-01-preview\"\n",
    "openai.api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\") \n",
    "# Comment these two lines out if using keys, set your API key in the OPENAI_API_KEY environment variable and set openai.api_type = \"azure\" instead\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# Set up clients for Cognitive Search and Storage\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=AzureKeyCredential(AZURE_SEARCH_SERVICE_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT uses a particular set of tokens to indicate turns in conversations\n",
    "prompt_prefix = \"\"\"<|im_start|>system\n",
    "어시스턴트는 검색 결과에서 나온 문서에 대한 질문을 도와줍니다. 답변은 간결하게 작성하세요.\n",
    "검색결과에 나오지 않은 내용은 답변하지 않습니다. 검색결과에 나온 내용을 잘 읽어보세요.\n",
    "표 형식의 정보는 HTML 테이블로 반환합니다. 마크다운 형식은 반환하지 마세요.\n",
    "각 출처에는 이름 뒤에 콜론과 실제 정보가 있으며, 응답에 사용하는 각 사실에 대한 출처 이름을 항상 포함하세요. 소스를 참조할 때는 대괄호를 사용합니다(예: [info1.txt]). 소스를 결합하지 말고 각 소스를 개별적으로 나열합니다(예: [info1.txt][info2.pdf]).\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "<|im_end|>\"\"\"\n",
    "\n",
    "turn_prefix = \"\"\"\n",
    "<|im_start|>user\n",
    "\"\"\"\n",
    "\n",
    "turn_suffix = \"\"\"\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt_history = turn_prefix\n",
    "\n",
    "history = []\n",
    "\n",
    "summary_prompt_template = \"\"\"아래는 지금까지의 대화 요약과 사용자가 검색결과에서 검색하여 답변해야 하는 새로운 질문입니다. 대화와 새 질문을 기반으로 검색 쿼리를 생성합니다.\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching: 인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "-------------------\n",
      "\n",
      "\n",
      "-------------------\n",
      "Prompt:\n",
      "<|im_start|>system\n",
      "어시스턴트는 검색 결과에서 나온 문서에 대한 질문을 도와줍니다. 답변은 간결하게 작성하세요.\n",
      "검색결과에 나오지 않은 내용은 답변하지 않습니다. 검색결과에 나온 내용을 잘 읽어보세요.\n",
      "표 형식의 정보는 HTML 테이블로 반환합니다. 마크다운 형식은 반환하지 마세요.\n",
      "각 출처에는 이름 뒤에 콜론과 실제 정보가 있으며, 응답에 사용하는 각 사실에 대한 출처 이름을 항상 포함하세요. 소스를 참조할 때는 대괄호를 사용합니다(예: [info1.txt]). 소스를 결합하지 말고 각 소스를 개별적으로 나열합니다(예: [info1.txt][info2.pdf]).\n",
      "Sources:\n",
      "dbr_sample-2.pdf:  압축 성장을 이루는 동시에 독자적인 비즈니스 모델을 갖추기까지 어떤 시행착오를 거쳤을까. DBR(동아비즈니스리뷰)이 아우름플래닛을 창업하고 현재는 미국 법인 LINER Inc.의 대표를 맡고 있는 김진우 대표를 만나 경쟁력 소프트 엣지’ 등 브라우저에서 라이너 확장 프로그램을 설치하고 구글, 네이버 등에서 검색을 하면 사용자와 관련성 높은 검색 결과가 큐레이션 된다. 올해 4월에는 GPT-4를 적용한 인공지능(AI) 검색용 챗봇 ‘라이너챗’을 출시했다. 최신 정보까지 소스로 활용하고 사용자 맞춤으로 답변을 제공한다. 라이너 웹 사이트나 앱에 접속하지 않아도 검색 포털과 연동돼 바로 활용할 수 있다는 점도 편리하다. 검색 결과 페이지에서 텍스트를 드래그하면 해당 내용을 저장할 수도 있고 번역, 추가 검색도 바로 할 수 있다. 라이너 앱이나 웹 사이트에서는 자체적인 검색 페이지와 라이너챗, 추천 콘텐츠 피드를 제공한다. 라이너가 초개인화된 추천 서비스를 제공할 수 있는 원천은 사용자들의 ‘형광펜’ 데이터다. 사실 라이너 자체가 인터넷에서 활용 가능한 형광펜 서비스로 시작했다. 책에서 중요한 부분을 형광펜으로 표시하듯 라이너는 인터넷에서도 쓸 수 있는 형광펜을 만들었다. 이를 빅데이터로 활용해 개인화된 검색 및 추천 서비스를 제공하며 전 세계 1000만 명 이상의 월간 활성화 이용자 수(MAU)를 모았다. 국내 페이스북 2017~20192020202120222023Downloaded by news123(DIGITAL), sh89.kang@kt.com, 2023-06-0279  DBR No. 370수는 1, 2명 수준에 불과했다. 이미 한국 시장에 맞춰 개발된 서비스로 단순히 서비스를 번역하는 것만으로는 글로벌 사용자의 호응을 얻기에 충분하지 않았던 것이다. 아이노갤러리를 정리하고 근본적으로 글로벌 시장에서 먹힐 수 있는 새로운 서비스를 발굴하기로 했다. 2015년, 두 창업자는 그간 아이노갤러리를 통해 번 돈 약 4200만 원을 들고 미국 실리콘밸리로 갔다.\n",
      "dbr_sample-4.pdf: 초개인화 추천 서비스가 된 형광펜고객을 직접 찾아가는 AI 연구 보조이렇게 인터넷 형광펜으로 시작한 라이너는 현재 효과적·효율적인 리서치를 위한 AI 툴로 구글 웹스토어, 마이크로소프트 엣지 추가 기능 페이지에서 라이너를 설치하고, 저장하고 싶은 텍스트를 드래그하면 형광펜 기능이 활성화된다. 여러 색을 적용해 정보의 유형, 중요도 등을 구분하고, 해당 정보는 라이너 웹 사이트, 앱에서 모아볼 수 있다출처: 동아닷컴 캡처Downloaded by news123(DIGITAL), sh89.kang@kt.com, 2023-06-0281  DBR No. 370그림 2 라이너 AI 서비스의 사용 횟수와 생성 단어 수 어를 제시한다. 구글 검색 결과를 소스로 함께 활용하기 때문에 기존에 챗GPT에서는 찾기 어려웠던 2021년 이후 정보까지 탐색할 수 있다. 챗봇은 페이지의 내용을 요약하거나 핵심 내용을 뽑아준다. 이해하기 어렵고 복잡한 문장을 쉽게 풀어주기도 하고 번역도 해준다. 추가 정보도 바로 검색할 수 있다. 사용자들을 통해 156가지 언어를 학습했고, 정교하게 구사할 수 있는 언어는 20개가 넘는다. 초개인화된 정보를 제공하기 위해 대화 내용을 학습하고 기억한다. 처음 라이너챗 출시 이후 열흘 만에 다운로드 수가 10만 건을 기록했고 일간 사용자 수는  44%가량 증가했다. 2023년 5월 17일 기준 사용 횟수는 약 3800만 회, 생성 단어는 약 470억 개에 달하며 현재도 가파른 우상향 곡선을 그리고 있다. 무엇보다 초개인화된 검색 결과를 제공한다는 게 라이너가 주장하는 차별점이다. 라이너는 사용자들의 하이라이트 정보를 기반으로 각자에게 필요한 검색 결과를 선보인다. PBL(Pick By Liner)은 검색 결과 추천 기능이다. 라이너 ‘검색 어시스턴트’를 활성화하면 사용자와 연관성 높은 정보 왼쪽에는 파란색 띠가 표시되는 형식이다. 라이너는 사용자들이 하이라이트한 약 2억6000만 개의 문서를 데이터로 축적했다. 하이라이트 데이터를 기반으로 서로 유사하다고 평가되는 사용자들이 많이 본 검색 결과를 토대로 해당 페이지나 답변을 제공하는 식이다.\n",
      "dbr_sample-3.pdf:  아이노갤러리를 정리하고 근본적으로 글로벌 시장에서 먹힐 수 있는 새로운 서비스를 발굴하기로 했다. 2015년, 두 창업자는 그간 아이노갤러리를 통해 번 돈 약 4200만 원을 들고 미국 실리콘밸리로 갔다. 전 세계에서 가장 많은 혁신이 탄생하는 지역에서 직접 부딪히며 전 세계에서 통할 혁신 아이디어를 찾겠다는 것이었다. 에어비앤비를 통해 숙소 겸 사무실을 구했다. 예산은 한정돼 있었고 어떤 서비스가 먹힐지는 오리무중이었다. 아우름플래닛은 가장 스타트업다운 방식으로 문제를 돌파했다. 1주일에 1개씩 서비스를 출시하는 강행군을 하기로 결심한 것이었다. 던져놓은 서비스 중 시장에서 반응이 오는 것들을 키우기로 했다. 김 대표가 개발, 우 대표가 디자인을 맡았던 이 프로젝트의 아이디어 수는 총 108개에 달했다. 이 중 칭찬 릴레이 커뮤니티 ‘티키타카’, 간단하게 자료를 전달하는 클라우드 ‘아이센드’ 등 8개 서비스를 론칭했다.라이너는 그중 3번째 서비스다. 김 대표는 평소 활자 중독 수준으로 읽을 것을 늘 손에 쥐고 있었다. 그러다 중요한 내용을 발견하면 바로 바지에서 형광펜을 꺼내 줄을 그었다. 김 대표는 인터넷에는 형광펜이 없는 게 늘 아쉬웠다. 검색 시장에도 불편함을 느꼈다. 과제를 위해 검색하다 상위에 뜬 페이지를 살펴봐도 원하는 정보가 없는 경우가 허다했다. 문득 누군가 정말 중요한 정보를 형광펜으로 표시해주면 좋겠다는 생각을 했다. 그러기 위해선 우선 인터넷에서 사용할 수 있는 형광펜이 필요했다. 그렇게 2015년 7월 형광펜 기능이 탑재된 iOS 브라우저 앱 ‘라이너’를 만들었다. 브라우있는 AI 서비스를 만들고, 스타트업이 지속가능한 성장을 이룰 수 있는 방법을 들었다. 무작정 떠난 실리콘밸리1주일에 1개씩 서비스 개발해아우름플래닛은 12년 차 스타트업이다. 연세대에서 컴퓨터과학을 전공한 김 대표와 같은 학교에서 불어불문학·경영학을 전공한 우찬민 아우름플래닛 대표는 연세대·고려대 연합 창업 동아리 인사이더스의 창립 멤버로 활동하며 창업의 꿈을 키웠다. 함께 창업 공모전에 나가 상도 타오는 등 손발을 맞추고 서로의 합을 확인했다.\n",
      "\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "user: 인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "-------------------\n",
      "assistant: 라이너는 인터넷에서 사용할 수 있는 형광펜 서비스입니다. 사용자가 웹 페이지나 문서에서 중요한 내용을 형광펜으로 표시할 수 있고, 이를 저장하거나 다른 사람과 공유할 수 있습니다. 이를 기반으로 라이너는 개인화된 검색 및 추천 서비스를 제공하며, 전 세계 1000만 명 이상의 월간 활성화 이용자 수(MAU)를 가지고 있습니다. 또한 GPT-4를 적용한 인공지능(AI) 검색용 챗봇 ‘라이너챗’을 출시하여 최신 정보까지 소스로 활용하고 사용자 맞춤으로 답변을 제공합니다. 검색 결과 페이지에서 텍스트를 드래그하면 해당 내용을 저장할 수도 있고 번역, 추가 검색도 바로 할 수 있습니다. 라이너는 사용자들이 하이라이트한 약 2억6000만 개의 문서를 데이터로 축적하여, 해당 내용을 기반으로 초개인화된 검색 결과를 제공합니다. PBL(Pick By Liner)은 검색 결과 추천 기능으로, 사용자와 연관성 높은 정보를 왼쪽 파란색 띠가 표시되는 형식으로 제공됩니다.\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell multiple times updating user_input to accumulate chat history\n",
    "user_input = \"인터넷 형광펜 서비스 소개좀 부탁해요\"\n",
    "\n",
    "# Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
    "exclude_category = None\n",
    "\n",
    "if len(history) > 0:\n",
    "    completion = openai.Completion.create(\n",
    "        engine=AZURE_OPENAI_GPT_DEPLOYMENT,\n",
    "        prompt=summary_prompt_template.format(summary=\"\\n\".join(history), question=user_input),\n",
    "        temperature=1,\n",
    "        max_tokens=32,\n",
    "        stop=[\"\\n\"])\n",
    "    search = completion.choices[0].text\n",
    "else:\n",
    "    search = user_input\n",
    "\n",
    "# Use Azure OpenAI to compute an embedding for the query\n",
    "query_vector = openai.Embedding.create(engine=AZURE_OPENAI_EMB_DEPLOYMENT, input=search)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "print(\"Searching:\", search)\n",
    "print(\"-------------------\")\n",
    "filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
    "\n",
    "# Hybrid search with semantic search and vector reranking\n",
    "\n",
    "r = search_client.search(search, \n",
    "                         filter=filter,\n",
    "                         query_type=QueryType.SEMANTIC, \n",
    "                         query_language=\"en-us\", \n",
    "                         query_speller=\"lexicon\", \n",
    "                         semantic_configuration_name=\"default\", \n",
    "                         top=3,\n",
    "                         vector=Vector(value=query_vector, k=50, fields=\"embedding\") if query_vector else None)\n",
    "\n",
    "# Vector search only\n",
    "# r = search_client.search(search, top=3, vector=Vector(value=query_vector, k=50, fields=\"embedding\") if query_vector else None)\n",
    "\n",
    "results = [doc[KB_FIELDS_SOURCEPAGE] + \": \" + doc[KB_FIELDS_CONTENT].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in r]\n",
    "content = \"\\n\".join(results)\n",
    "\n",
    "prompt = prompt_prefix.format(sources=content) + prompt_history + user_input + turn_suffix\n",
    "\n",
    "print(\"\\n-------------------\\n\".join(history))\n",
    "print(\"\\n-------------------\\nPrompt:\\n\" + prompt)\n",
    "\n",
    "completion = openai.Completion.create(\n",
    "    engine=AZURE_OPENAI_CHATGPT_DEPLOYMENT, \n",
    "    prompt=prompt, \n",
    "    temperature=0.7, \n",
    "    max_tokens=1024,\n",
    "    stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
    "\n",
    "prompt_history += user_input + turn_suffix + completion.choices[0].text + \"\\n<|im_end|>\" + turn_prefix\n",
    "history.append(\"user: \" + user_input)\n",
    "history.append(\"assistant: \" + completion.choices[0].text)\n",
    "\n",
    "print(\"\\n-------------------\\n\".join(history))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40b9fc8dfc687e53ddb074d322e19207ef9cf3db51c580aef67976913dea803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

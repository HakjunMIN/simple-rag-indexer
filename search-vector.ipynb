{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType, Vector\n",
    "\n",
    "load_dotenv()\n",
    "# Replace these with your own values, either in environment variables or directly here\n",
    "AZURE_STORAGE_ACCOUNT = os.environ.get(\"AZURE_STORAGE_ACCOUNT\")\n",
    "AZURE_STORAGE_CONTAINER = os.environ.get(\"AZURE_STORAGE_CONTAINER\")\n",
    "AZURE_SEARCH_SERVICE = os.environ.get(\"AZURE_SEARCH_SERVICE\")\n",
    "AZURE_SEARCH_INDEX = os.environ.get(\"AZURE_SEARCH_INDEX\") \n",
    "AZURE_OPENAI_SERVICE = os.environ.get(\"AZURE_OPENAI_SERVICE\") \n",
    "AZURE_OPENAI_GPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_GPT_DEPLOYMENT\") \n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\") \n",
    "AZURE_OPENAI_EMB_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_EMB_DEPLOYMENT\")\n",
    "AZURE_SEARCH_SERVICE_KEY = os.environ.get(\"AZURE_SEARCH_SERVICE_KEY\") \n",
    "\n",
    "KB_FIELDS_CONTENT = os.environ.get(\"KB_FIELDS_CONTENT\") or \"content\"\n",
    "KB_FIELDS_CATEGORY = os.environ.get(\"KB_FIELDS_CATEGORY\") or \"category\"\n",
    "KB_FIELDS_SOURCEPAGE = os.environ.get(\"KB_FIELDS_SOURCEPAGE\") or \"sourcepage\"\n",
    "\n",
    "\n",
    "# Used by the OpenAI SDK\n",
    "openai.api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "openai.api_version = \"2023-06-01-preview\"\n",
    "openai.api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\") \n",
    "# Comment these two lines out if using keys, set your API key in the OPENAI_API_KEY environment variable and set openai.api_type = \"azure\" instead\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# Set up clients for Cognitive Search and Storage\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=AzureKeyCredential(AZURE_SEARCH_SERVICE_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT uses a particular set of tokens to indicate turns in conversations\n",
    "prompt_prefix = \"\"\"<|im_start|>system\n",
    "어시스턴트는 에 정보가 충분하지 않은 경우에는 모른다고 말하검색 결과에서 나온 문서에 대한 질문을 도와줍니다. 답변은 간결하게 작성하세요.\n",
    "\n",
    "표 형식의 정보는 HTML 테이블로 반환합니다. 마크다운 형식은 반환하지 마세요.\n",
    "각 출처에는 이름 뒤에 콜론과 실제 정보가 있으며, 응답에 사용하는 각 사실에 대한 출처 이름을 항상 포함하세요. 소스를 참조할 때는 대괄호를 사용합니다(예: [info1.txt]). 소스를 결합하지 말고 각 소스를 개별적으로 나열합니다(예: [info1.txt][info2.pdf]).\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "<|im_end|>\"\"\"\n",
    "\n",
    "turn_prefix = \"\"\"\n",
    "<|im_start|>user\n",
    "\"\"\"\n",
    "\n",
    "turn_suffix = \"\"\"\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt_history = turn_prefix\n",
    "\n",
    "history = []\n",
    "\n",
    "summary_prompt_template = \"\"\"아래는 지금까지의 대화 요약과 사용자가 검색결과에서 검색하여 답변해야 하는 새로운 질문입니다. 대화와 새 질문을 기반으로 검색 쿼리를 생성합니다.\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching: 인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "-------------------\n",
      "\n",
      "\n",
      "-------------------\n",
      "Prompt:\n",
      "<|im_start|>system\n",
      "어시스턴트는 에 정보가 충분하지 않은 경우에는 모른다고 말하검색 결과에서 나온 문서에 대한 질문을 도와줍니다. 답변은 간결하게 작성하세요.\n",
      "\n",
      "표 형식의 정보는 HTML 테이블로 반환합니다. 마크다운 형식은 반환하지 마세요.\n",
      "각 출처에는 이름 뒤에 콜론과 실제 정보가 있으며, 응답에 사용하는 각 사실에 대한 출처 이름을 항상 포함하세요. 소스를 참조할 때는 대괄호를 사용합니다(예: [info1.txt]). 소스를 결합하지 말고 각 소스를 개별적으로 나열합니다(예: [info1.txt][info2.pdf]).\n",
      "Sources:\n",
      "dbr_sample-3.pdf: 이중칭찬릴레이커뮤니티‘티키타카’ , 간단하게자료 를전달 하 는클라 우 드‘아이센드’ 등8개서비스 를론칭했다.은라이너는그중3번째서비스다.김대표는평소활 자중 독수준으로읽을것을늘손에쥐고 있었다.그러다중요한내용을발견하면바로 바지에서형광펜을꺼내줄을그었다.김대표 는인터넷에는형광펜이없는게늘아쉬웠다. 검색시장에도불편함을느꼈다.과제를위해 검색하다상위에뜬페이지를살펴봐도원하 정보가없는경우가허다했다.문득누군가정는 말중요한정보를형광펜으로표시해주면좋겠 다는생각을했다.그러기위해선우선인터넷 에서사 할수있는형광펜이필요했다. 용그렇게2015년7월형광펜기능이탑재된 iOS브라우저앱‘라이너’를만들었다.브라우79 DBR No. 370Downloaded by news123(DIGITAL), sh89.kang@kt.com, 2023-06-02 출처:동아닷컴캡처구글웹스토어,마이크로소프트엣지추가기능페이지에서라이너를설치하고,저장하고싶은텍스트를 드래그하면형광펜기능이활성화된다.여러색을적용해정보의유형,중요도등을구분하고,해당정보는 라이너웹사이트,앱에서모아볼수있다저에서정보를탐색하다중요한텍스트를드래 그하면형광펜버튼이활성화되는식이다.하 이라이트한정보들 는을한데모아다시검색하 불편함없이찾아볼수있게했다.사실라이너 에는큰기대가없었다.너무단순한아이디어 라사업이될지의문이었다.그러나시장의반 응 은달랐다.출시첫날에400건이다운로드됐 다.반짝사람들의관심을끈줄알았는데출시 한달만5000건을달성했다.라이너의가능성 이입증된것이다.라이너의다음미션은꾸준히앱을찾 는사 용자의비율,즉사 용 자유지율(리텐션)을높게유 지하는것이었다.다운로드한사람은많은데 상사막 용하는이가없다면‘유령앱’이나다름없다.보통사 용 용 자유지 자수가적을때는사율이높게나타난다.사용목적이뚜렷한이용 자들이얼리어댑터로유입되기때문이다.꾸준용 자유지율 을끌어올 히몸집을키우면서도사려야충성도높 은사용자들 을바탕으로비즈니스모델도접목 할수있다.사 람 들이매일라이너를찾게만 들 기위해택한 름아닌데스것은다 크 톱중 심제품 으 로의전환이 다.라이너를처음출 시한지2달만에구 글 의브라장프로그램버전을출시했다.\n",
      "dbr_sample-4.pdf: 꾸준용 자유지율 을끌어올 히몸집을키우면서도사려야충성도높 은사용자들 을바탕으로비즈니스모델도접목 할수있다.사 람 들이매일라이너를찾게만 들 기위해택한 름아닌데스것은다 크 톱중 심제품 으 로의전환이 다.라이너를처음출 시한지2달만에구 글 의브라장프로그램버전을출시했다. 우저인크 롬의확장프로그램은말그대로브라 山 6. AI 것 확 우저에광고차단,번역,일정관리등부 가적인기능 을더하 는프 로 그램으 로구 글웹스 토어등에서다 운 로 드받 을 수있다.김대표 는“당 시모 두 가모 바일을외치던 때라시대에역행하 는시도였다”며“그러나미국 과한 국에서정보탐 색방 식의차이가있었다”라 고설명했다.대중 람 들 를교 통이발달 한한 국에선사 이이동 하며모바일로긴글을읽는다.반면차 람 들이모 바일로더많이타고다니는미국에선사는중 요한내용 을잘찾아 보지않 는다.길에서핸 폰 으드로검색하기보단스타벅스에서노트 북을펴고검색하는게더일반적이었다.사 크용 자특성 톱버전이필요 했다.하이라이을고려해도데스팅을할정도 용 로‘각’을잡색하 는사 고정보 를탐 자라면모바일보단PC검색을더선호 할것이다.결과크 톱중 심제품 로의전환 적으 로데스 으은옳은선택이었다.일간사용 자유지율이80%이상 로고정됐다.으비자 상미국에서최대한머물수있는기간3 달정도를지냈지만돈은벌지못했다.미국으 로쥐고간4200만원중고작10만원이남았 다.대신글로벌시장에서승부수를띄울서비 스,라이너를찾아낸것이큰성과였다.초 개인화추천서비스 가된형광펜고 객을직접찾아 가는AI연구보조이렇게인터넷형광펜으로시작한라이너는 현재효과적·효율적인리서치를위한AI툴로한국형발사체 '누리호(KSLV- Ⅱ)'가 25일 세 번째 발사에 성공하며 국내 개발진이 만든 위성 을 궤도에 안착시키는 임무를 처음으로 완료했다. 우리 손으로 만든 발사체에 실제 사용할 국내 위성을 실어 보낸 것은 사상 처음이다.누리호는 이날 오후 6시 24분 전남 고흥군 나로우주센터에서 발사됐다. 폭발음을 내며 솟구 쳐 오른 누리호는 123초 뒤 1단, 267초 뒤 2단 분리에 각각 성공했다. 이후 고도 550km 궤 도에 진입한 누리호는 주탑재위성인 차세대 소형위성 2호' 다.\n",
      "dbr_sample-0.pdf: PDF EditionTIMELESS INSIGHTDDBRDong-A Business Review dbr.donga.comDBR Case Study:인터넷형광펜에서AI서비스로성장한‘라이너(LINER)’ 검색정확 성높여준‘초개인화데이터’ 효과적리서치돕는AI툴로자리잡아저작권공지본PDF문서에실린글,그림,사진등저작권자가표시되어있지않은모든자료는발행사인㈜동아일보 사전동의없이는어떠한경우에도사용할수없습니다.사에저작권이있으며,무단전재재배포금지 본PDF문서는DBR독자및dbr.donga.com회원에게㈜동아일보사가제공하는것으로저작권법의보호를받습니다.㈜동아일보 사의허락없이PDF문서를온라인사이트등에무단게재,전재하거나유포할수없습니다.본파일중일부기능은제한될수있습니다.구독문의●개인구독문의: 02-6718-7802은행계좌:우리은행1005-801-116229㈜디유넷●단 체구독문의: 02-361-1506 은행계좌:국민은행009-01-1307-189㈜동아일보사동아일보사Downloaded by news123(DIGITAL), sh89.kang@kt.com, 2023-06-02 DBR Case Study:인터넷형광펜에서AI서비스로성장한‘라이너(LINER)’검색정확성높여준‘초개인화데이터’ 효과적리서치돕는AI툴로자리잡아이규열기자kylee@donga.comArticle at a Glance2015년형광펜기능을제공하는브라우저앱및브라우저확장프로그램으로출시된‘라 이너(LINER)’는사용자들의하이라이트데이터를토대로개인화된검색결과및콘텐츠 를큐레이션하는AI서비스로성장했다. GPT-4를적용한검색용챗봇‘라이너챗’은사용 자맞춤답변을내놓는다.라이너는‘가설공장’이라는자체적인프로세스를구축해2주 동안제품을개선할아이디어를개발,실험하며새로운기능을더해나갔다.별도의웹사이 트에접속할필요없이구글,네이버등기존의검색환경에서사용할수있도록설계한점 도편리성을더했다.라이너는해외사용자가90%에달하는글로벌서비스로전체MAU 는1000만명이상이다.미국,한국등에서는구독을,인도,동남아등에서는광고수익을 내세워국가별사용자특색에따른비즈니스모델을구축했다.\n",
      "\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "user: 인터넷 형광펜 서비스 소개좀 부탁해요\n",
      "-------------------\n",
      "assistant: 인터넷 형광펜 서비스는 사용자가 인터넷 상에서 원하는 텍스트를 강조하거나 하이라이트 처리할 수 있는 서비스입니다. 이를 통해 사용자는 중요한 내용을 더 쉽게 찾을 수 있고, 필요한 내용을 쉽게 기억할 수 있습니다. 최근에는 인공지능 기술을 활용한 라이너(LINER)와 같은 서비스도 등장해, 개인화된 검색 결과와 콘텐츠를 제공하고 있습니다. 이를 통해 사용자는 더욱 쉽게 필요한 정보를 찾을 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell multiple times updating user_input to accumulate chat history\n",
    "user_input = \"인터넷 형광펜 서비스 소개좀 부탁해요\"\n",
    "\n",
    "# Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
    "exclude_category = None\n",
    "\n",
    "if len(history) > 0:\n",
    "    completion = openai.Completion.create(\n",
    "        engine=AZURE_OPENAI_GPT_DEPLOYMENT,\n",
    "        prompt=summary_prompt_template.format(summary=\"\\n\".join(history), question=user_input),\n",
    "        temperature=1,\n",
    "        max_tokens=32,\n",
    "        stop=[\"\\n\"])\n",
    "    search = completion.choices[0].text\n",
    "else:\n",
    "    search = user_input\n",
    "\n",
    "# Use Azure OpenAI to compute an embedding for the query\n",
    "query_vector = openai.Embedding.create(engine=AZURE_OPENAI_EMB_DEPLOYMENT, input=search)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "print(\"Searching:\", search)\n",
    "print(\"-------------------\")\n",
    "filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
    "\n",
    "# Hybrid search with semantic search and vector reranking\n",
    "\n",
    "r = search_client.search(search, \n",
    "                         filter=filter,\n",
    "                         query_type=QueryType.SEMANTIC, \n",
    "                         query_language=\"en-us\", \n",
    "                         query_speller=\"lexicon\", \n",
    "                         semantic_configuration_name=\"default\", \n",
    "                         top=3,\n",
    "                         vector=Vector(value=query_vector, k=50, fields=\"embedding\") if query_vector else None)\n",
    "\n",
    "# Vector search only\n",
    "# r = search_client.search(search, top=3, vector=Vector(value=query_vector, k=50, fields=\"embedding\") if query_vector else None)\n",
    "\n",
    "results = [doc[KB_FIELDS_SOURCEPAGE] + \": \" + doc[KB_FIELDS_CONTENT].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in r]\n",
    "content = \"\\n\".join(results)\n",
    "\n",
    "prompt = prompt_prefix.format(sources=content) + prompt_history + user_input + turn_suffix\n",
    "\n",
    "print(\"\\n-------------------\\n\".join(history))\n",
    "print(\"\\n-------------------\\nPrompt:\\n\" + prompt)\n",
    "\n",
    "completion = openai.Completion.create(\n",
    "    engine=AZURE_OPENAI_CHATGPT_DEPLOYMENT, \n",
    "    prompt=prompt, \n",
    "    temperature=0.7, \n",
    "    max_tokens=1024,\n",
    "    stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
    "\n",
    "prompt_history += user_input + turn_suffix + completion.choices[0].text + \"\\n<|im_end|>\" + turn_prefix\n",
    "history.append(\"user: \" + user_input)\n",
    "history.append(\"assistant: \" + completion.choices[0].text)\n",
    "\n",
    "print(\"\\n-------------------\\n\".join(history))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40b9fc8dfc687e53ddb074d322e19207ef9cf3db51c580aef67976913dea803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
